# HPA Example

This example demonstrates how to deploy a simple **GeoserverCloud** setup in your local cluster, allowing you to evaluate and understand the behavior of **Horizontal Pod Autoscaling (HPA)**.

## Setup Overview

The deployment includes the following components:

- **WebUI instance**: Provides access to the catalog configuration.
- **Gateway**: Serves as the entry point to the GeoserverCloud solution.
- **Two WMS instances**: These initial instances serve the WMS OGC protocol.
- **REST API instance**: Utilized by a script to create a minimal catalog for testing.
- **Local PostgreSQL**: Used with the PgConfig profile to minimize startup time by focusing on efficient catalog reads.
- **RabbitMQ**: Manages event communication across instances.

By following the steps in the next section, you will observe how HPA automatically scales the deployment (up to 100 containers!) in response to stress generated by the provided script.

## Considerations

- Make sure to review the documentation in the `../README.md` file. A local Kubernetes cluster and `kubectl` are required to run this demo.
- Be aware that running these tests may overload or freeze your machine if your hardware or configuration is inadequate.
- If needed, adjust the **maxReplicas** value in the `values.yaml` file under the HPA section. The default value is set to 100.

## Steps to Run the Example

At the root level of the repository, follow these steps:

### 1. Execute the example command:

```shell
make example-pgconfig-wms-hpa
```

### 2. Verify that all pods are running:

```shell
kubectl get pods
```

Ensure all pods have the status `STATUS = Running` and `READY = 1/1`. Example output:

```shell
NAME                                                     READY   STATUS    RESTARTS   AGE
gs-cloud-pgconfig-wms-hpa-gsc-gateway-76b46b9c7f-gs976   1/1     Running   0          12m
gs-cloud-pgconfig-wms-hpa-postgresql-0                   1/1     Running   0          12m
gs-cloud-pgconfig-wms-hpa-gsc-rest-7fdbcf799f-qshn5      1/1     Running   0          12m
gs-cloud-pgconfig-wms-hpa-gsc-webui-6cf8f88695-646xt     1/1     Running   0          12m
gs-cloud-common-rabbitmq-0                               1/1     Running   0          12m
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-qs946       1/1     Running   0          11m
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-pth59       1/1     Running   0          10m
```

### 3. Define a DNS alias:

This step prevents the use of local references in the scripts creating an entry like the following in `/etc/hosts`

```shell
192.168.208.2	gscloud.local
```

You can modify the scripts if needed.

```shell
kubectl get ingress --no-headers gs-cloud-pgconfig-wm-geoserver-host1 | \
awk '{printf("%s\t%s\n", $4, $3)}' | sudo tee -a /etc/hosts
```

### 4. Initialize the catalog:

Run the following command to create a test layer:

```shell
/examples/pgconfig-wms-hpa/init-catalog.sh
```

Should output:

```shell
------------------------------------
Preparing initialization process ...
------------------------------------
Creating workspace 'hpa-test'...
hpa-test
------------------------------------
Creating WMS datastore 'swisstopo_wms'...
swisstopo_wms
------------------------------------
Publishing layer 'ch.bafu.grundwasserkoerper' from datastore 'swisstopo_wms'...
ch.bafu.grundwasserkoerper
------------------------------------
Catalog initialized successfully.
```

### 5. Monitor the pods:

Use the following command to continuously monitor the pod status (refreshes every second).
This will let you see the number of `wms` pods increase during stress testing, and decrease back to one pod after a while.

```shell
watch -n 1 kubectl top pod -l app.kubernetes.io/component=wms --sort-by cpu
```

Initially it will output something like this, with the only wms pod running by default:

```shell
Every 1.0s: kubectl top pod -l app.kubernetes.io/component=wms --sort-by cpu

NAME                                                 CPU(cores)   MEMORY(bytes)
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-2frh9   3m           533Mi
```

The `kubectl get hpa` command is also useful to monitor

```shell
watch -n 1 kubectl get hpa gs-cloud-pgconfig-wms-hpa-gsc-wms

Every 1.0s: kubectl get hpa gs-cloud-pgconfig-wms-hpa-gsc-wms
NAME                                REFERENCE                                      TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
gs-cloud-pgconfig-wms-hpa-gsc-wms   Deployment/gs-cloud-pgconfig-wms-hpa-gsc-wms   3%/70%    1         16        16         23m
```

### 6. Stress test the server:

> [!NOTE]
> The `stress-server.sh` script uses Apache Benchmark (`ab` command). If you don't have it, install it with
>
> ```shell
> sudo apt-get install apache2-utils
> ```
>
> or with your Operating System's package manager.

In a new terminal window, run the stress test script. It'll run for 60 seconds making 100 concurrent WMS requests to the cluster:

```shell
./examples/pgconfig-wms-hpa/stress-server.sh
```

You will observe in the first terminal how the number of pods scales up and down dynamically during the stress test.

For example, at some point the number of `wms` pods scaled to 4 here as the CPU utilization increased:

```shell
Every 1.0s: kubectl top pod -l app.kubernetes.io/component=wms --sort-by cpu

NAME                                                 CPU(cores)   MEMORY(bytes)
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-h8nmg   1888m        818Mi
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-xvr2r   1532m        867Mi
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-9kp2g   1318m        845Mi
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-nb2cc   1305m        891Mi
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-9hhlb   1285m        882Mi
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-zpk4d   1267m        860Mi
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-wmdxw   1102m        1398Mi
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-lrlv8   1027m        834Mi
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-6nv2j   968m         826Mi
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-wmvjh   901m         821Mi
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-dtx4k   865m         1301Mi
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-9s4fk   584m         1447Mi
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-gxbj4   460m         985Mi
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-2frh9   367m         953Mi
```

And with `kubectl get hpa` will observe the status of the **HorizontalPodAutoscaler**, similar to:

```shell
watch -n 1 kubectl get hpa gs-cloud-pgconfig-wms-hpa-gsc-wms

Every 1.0s: kubectl get hpa gs-cloud-pgconfig-wms-hpa-gsc-wms

NAME                                REFERENCE                                      TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
gs-cloud-pgconfig-wms-hpa-gsc-wms   Deployment/gs-cloud-pgconfig-wms-hpa-gsc-wms   941%/70%   1         16        14         4m28s
```

Here, CPU consumption has increased to 941% out of the 70% average utilization configured. As a result, the Deployment was resized to 14 replicas:

```shell
kubectl get deployment gs-cloud-pgconfig-wms-hpa-gsc-wms
NAME                                READY   UP-TO-DATE   AVAILABLE   AGE
gs-cloud-pgconfig-wms-hpa-gsc-wms   14/14   14           14          9m23s
```

And after a couple minutes the replicas start to ramp down until reaching one:

```shell
kubectl get deployment gs-cloud-pgconfig-wms-hpa-gsc-wms
NAME                                READY   UP-TO-DATE   AVAILABLE   AGE
gs-cloud-pgconfig-wms-hpa-gsc-wms   1/1     1            1           9m25s

kubectl top pod -l app.kubernetes.io/component=wms --sort-by cpu
NAME                                                 CPU(cores)   MEMORY(bytes)
gs-cloud-pgconfig-wms-hpa-gsc-wms-758dfd8765-2frh9   4m           971Mi

kubectl get hpa gs-cloud-pgconfig-wms-hpa-gsc-wms
NAME                                REFERENCE                                      TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
gs-cloud-pgconfig-wms-hpa-gsc-wms   Deployment/gs-cloud-pgconfig-wms-hpa-gsc-wms   3%/70%    1         16        1          15m
```

### 7. Clean up the environment:

To remove the deployment and restore your environment, run:

```shell
make examples-clean
```
